{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 70. データの入手・整形"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずは必要となるpositive/negativeのデータセットをインストールします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash ../shell/setup_ch8_dataset.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ファイルが存在することを確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ls ../data/raw/rt-polaritydata/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ファイルの中身を見てみましょう。まずは rt-polarity.pos (ポジティブな文のデータセット) です"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 3 ../data/raw/rt-polaritydata/rt-polarity.pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次は rt-polarity.neg (ネガティブな文のデータセット) です"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 3 ../data/raw/rt-polaritydata/rt-polarity.neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 70-1 rt-polarity.posの各行の先頭に\"+1 \"という文字列を追加する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "さて1つ目のタスクです。\n",
    "\n",
    "エディタを開いてPythonコードを…とやりたくなるかもしれませんが、このくらいの加工ならターミナル上でワンライナーで書けるようになりましょう。\n",
    "\n",
    "Pythonのコードをいちいち書くよりも簡単に高速に書くことができるため、作業の効率化に繋がります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "求められていることは、 1つのファイル内の **各行で** 同様の処理を行うことです。このような場合は、 **awk** を 使います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "awkをインストールしましょう\n",
    "環境によりインストール方法が異なるので、各自調べてください。以下のコマンドが成功すればOKです"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!echo '' | awk '{print \"OK\"}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のワンライナーで、やりたいことを実現できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ../data/raw/rt-polaritydata/rt-polarity.pos | awk '{print \"+1 \"$0}' > ../data/preprocessing/rt-polarity.pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解説です。awkの基本構文は、\n",
    "\n",
    "```sh\n",
    "awk '{ (各行でやりたいこと) }'\n",
    "```\n",
    "\n",
    "です。Pythonで表現するとしたら、\n",
    "\n",
    "```py\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.rstrip()\n",
    "    (各行でやりたいこと)\n",
    "```\n",
    "\n",
    "ですね。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回のタスクでは、各行に対して先頭に `\"+1 \"` という文字列を追加して出力するということをしたいです。\n",
    "\n",
    "出力のために使う関数が `print()` です。\n",
    "\n",
    "`$0` は 「今見ている行 (文字列) 」です。\n",
    "\n",
    "文字列は横に並べると結合できるので、 `\"+1 \"$0` は「+1」「(スペース)」「今見ている行の文字列」を結合した文字列を意味します。\n",
    "\n",
    "この文字列を `print()` して、やりたいことが実現できるわけです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【ひとこと】\n",
    "\n",
    "インストールした圧縮ファイル (compression)、展開したファイル (raw)、システムへの入力となるファイル (preprocessing)のように、別のディレクトリに保存する習慣を付けましょう。\n",
    "\n",
    "元のファイルを上書きしてしまうと、別のやり方を試そうと思った時にもう一度インストールするところから始めなければいけません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 70-2 rt-polarity.posの各行の先頭に\"-1 \"という文字列を追加する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先ほどのやり方を応用して awk で書いてみましょう。\n",
    "\n",
    "以下のセルにシェルコマンドを書いてください。出力先は ../data/preprocessing/rt-polarity.neg です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!echo ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "確認します。\"OK\" が表示されれば成功しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! [[ -e ../data/preprocessing/rt-polarity.neg ]] && [[ $(grep -c -v '^-1 [^ ]' ../data/preprocessing/rt-polarity.neg) -ne 0 ]] && [[ $(cat ../data/raw/rt-polaritydata/rt-polarity.neg | wc -l) -eq $(cat ../data/preprocessing/rt-polarity.neg | wc -l) ]] && echo \"OK\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 70-3 posとnegのファイルを結合し、行をランダムに並び替える"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これもワンライナーですぐにできます。\n",
    "\n",
    "2つのファイルを **結合** するためには `cat` コマンドを使います。\n",
    "\n",
    "あるファイルの中身を **ランダムに並び替える** には、 `sort -R` コマンドを使います。\n",
    "\n",
    "これまでにも使っていましたが、シェルの機能である **パイプ** を使うと、上の2つの処理を続けて行うことができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 失敗します\n",
    "!cat ../data/preprocessing/rt-polarity.pos ../data/preprocessing/rt-polarity.neg | sort -R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# おまじないを付けて実行\n",
    "!cat ../data/preprocessing/rt-polarity.pos ../data/preprocessing/rt-polarity.neg | LC_ALL=C sort -R > ../data/preprocessing/sentiment.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 確認\n",
    "!head -n 5 ../data/preprocessing/sentiment.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正例の数の確認\n",
    "!grep \"^+1\"  ../data/preprocessing/sentiment.txt | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 負例の数の確認\n",
    "!grep \"^-1\"  ../data/preprocessing/sentiment.txt | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 71. ストップワード"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "※ 時間の関係で省略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 72. 素性抽出\n",
    "極性分析に有用そうな素性を各自で設計し,学習データから素性を抽出せよ.素性としては,レビューからストップワードを除去し,各単語をステミング処理したものが最低限のベースラインとなるであろう."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実際のタスクでは言語的な特徴をいかに捉えて素性として表現するかがポイントなのですが、今回はscikit-learnの関数でサクッと実現しましょう。\n",
    "\n",
    "単語の回数でベクトル化する CountVectorizer\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html?highlight=vectorizer#sklearn.feature_extraction.text.CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 別途書いておいた関数を呼べるように、パス追加\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# 書き換えてください\n",
    "project_home = '~/project/coworker-jp/nlp-knock-handson'\n",
    "if not os.path.exists(os.path.expanduser(project_home)):\n",
    "    raise RuntimeError('git cloneで作成したディレクトリのパスで書き換えてください')\n",
    "\n",
    "sys.path.append(project_home + '/jp/co/coworker/nlp_knock_handson/ch8/')\n",
    "\n",
    "%cd $project_home\n",
    "%pwd\n",
    "\n",
    "from jp.co.coworker.nlp_knock_handson.ch8.config import Config\n",
    "from jp.co.coworker.nlp_knock_handson.ch8.preprocessor import PreProcessor\n",
    "preprocessor = PreProcessor()\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "nltk.download('popular', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの読み込み\n",
    "texts, Y = PreProcessor.load_dataset(Config.get_dataset_path())\n",
    "\n",
    "# 単語の回数でベクトル化 (最も簡単な例)\n",
    "# decode_error='ignore' は、入力の単語にutf-8外の文字があった場合の設定。おまじない\n",
    "vectorizer = CountVectorizer(decode_error='ignore')\n",
    "X_simple = vectorizer.fit_transform(texts)\n",
    "\n",
    "feature_words = vectorizer.get_feature_names()\n",
    "print(feature_words[100:200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 英語のストップワード除去\n",
    "stop_words = preprocessor.get_stop_words()\n",
    "vectorizer = CountVectorizer(decode_error='ignore', stop_words=stop_words)\n",
    "X = vectorizer.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ステミング + プリセットで登録されているstop_wordsを使う\n",
    "stemmer = nltk.stem.lancaster.LancasterStemmer()\n",
    "print(stemmer.stem(\"studied\"))\n",
    "print(stemmer.stem(\"cats\"))\n",
    "\n",
    "# 単語分割\n",
    "print(nltk.word_tokenize(\"Hello, have a nice day.\"))\n",
    "\n",
    "def stem_analyzer(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return [stemmer.stem(token)  for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(decode_error='ignore', stop_words='english', analyzer=stem_analyzer)\n",
    "X = vectorizer.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_words = vectorizer.get_feature_names()\n",
    "print(feature_words[10:90])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 73. 学習\n",
    "72で抽出した素性を用いて,ロジスティック回帰モデルを学習せよ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression().fit(X_simple, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 74. 予測\n",
    "73で学習したロジスティック回帰モデルを用い,与えられた文の極性ラベル(正例なら\"+1\",負例なら\"-1\")と,その予測確率を計算するプログラムを実装せよ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = clf.predict(X_simple)\n",
    "\n",
    "prob_list = clf.predict_proba(X_simple)\n",
    "Y_pred_prob = [neg_pos[1] for neg_pos in prob_list]\n",
    "\n",
    "Y_pred_prob[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 75. 素性の重み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf.coef_ に 重みの情報が入っている\n",
    "# ( クラス数 *  素性数 ) の2次元配列\n",
    "weight_list = clf.coef_[0]\n",
    "print(weight_list)\n",
    "\n",
    "# 素性がどの単語を表しているかは、get_feature_names()で分かる\n",
    "feature_words = vectorizer.get_feature_names()\n",
    "print(feature_words[100:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_pairs = zip(weight_list, feature_words)\n",
    "sorted_weight_pairs = sorted(weight_pairs, key=lambda p: p[0])\n",
    "\n",
    "worst10 = sorted_weight_pairs[:10]\n",
    "print(worst10)\n",
    "\n",
    "top10 = sorted_weight_pairs[-10:]\n",
    "print(top10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 76. ラベル付け"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 整形するだけ\n",
    "a = zip(Y, Y_pred, Y_pred_prob, texts)\n",
    "\n",
    "for pairs in list(a)[:10]:\n",
    "  print(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 77. 正解率の計測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearnに関数がある\n",
    "precision = precision_score(Y, Y_pred)\n",
    "recall = recall_score(Y, Y_pred)\n",
    "f1 = f1_score(Y, Y_pred)\n",
    "\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 78. 5分割交差検定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/cross_validation.html\n",
    "\n",
    "scorers_dict = {\n",
    "    'precision': make_scorer(precision_score),\n",
    "    'recall': make_scorer(recall_score),\n",
    "    'f1': make_scorer(f1_score),\n",
    "}\n",
    "\n",
    "scores = cross_validate(clf, X, Y, scoring=scorers_dict)\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 79. 適合率-再現率グラフの描画"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, threshold = precision_recall_curve(Y, Y_pred_prob)\n",
    "\n",
    "plt.plot(precision, recall)\n",
    "plt.xlabel('Precision')\n",
    "plt.ylabel('Recall')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
